{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azKzys_66G42"
   },
   "source": [
    "# Kubeflow Pipelines Using Fashion MNIST\n",
    "\n",
    "[Kubeflow Pipelines](https://https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) is a machine learning workflow platform that is helping data scientists and ML engineers tackle experimentation and productionization of ML workloads. It allows users to easily orchestrate scalable workloads using an SDK right from the comfort of a Jupyter Notebook.\n",
    "\n",
    "[Microk8s](https://https://microk8s.io/docs) is a service that gives you the ability to spin up a lightweight Kubernetes cluster right on your local machine. It comes with Kubeflow built right in. \n",
    "\n",
    "This notebook will walk you through the steps of taking a current workload you may have and converting it to a notebook which can then be run using Kubeflow. \n",
    "\n",
    "This notebook features code from a tutorial which can found on the [Tensorflow website](https://https://www.tensorflow.org/tutorials/keras/classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kN5XA9ybEtwU"
   },
   "source": [
    "## Fashion MNIST dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJ4EgzcMEUko"
   },
   "source": [
    "The [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)  dataset contains 70,000 grayscale images in 10 clothing categories. Each image is 28x28 pixels in size. We chose this dataset to demonstrate the funtionality of Kubeflow Pipelines without introducing too much complexity in the implementation of the ML model.\n",
    "\n",
    "To familiarize you with the dataset we will do a short exploration. It is always a good idea to understand your data before you begin any kind of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMIxLNEiGR3x"
   },
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install packages for data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in ./.local/lib/python3.6/site-packages (20.1.1)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.0.5-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already up-to-date: matplotlib in ./.local/lib/python3.6/site-packages (3.2.2)\n",
      "Requirement already up-to-date: numpy in ./.local/lib/python3.6/site-packages (1.19.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.2)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in ./.local/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.2.0)\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.24.0, but you'll have pandas 1.0.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 0.24.2\n",
      "    Uninstalling pandas-0.24.2:\n",
      "      Successfully uninstalled pandas-0.24.2\n",
      "Successfully installed pandas-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "!pip install --user --upgrade pandas matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart kernel to allow installs to take effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y4DB_1u5H1fT",
    "outputId": "58a8d65c-e978-4af2-aa85-114d4a7f3f29"
   },
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Data exploration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OW5-_PC3H3YB"
   },
   "source": [
    "First we will download the dataset and split it into train and test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrDlsMh4LoXz"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9FDsUlxCaWW"
   },
   "source": [
    "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vec35x3cMQfo"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Y_88GQbMX1G"
   },
   "source": [
    "Let's look at the format of each dataset split. The training set contains 60,000 images and the test set contains 10,000 images which are each 28x28 pixels.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "e2-Zbu0BMpt1",
    "outputId": "51310a2d-ad13-4127-f1eb-db3237524044"
   },
   "outputs": [],
   "source": [
    "print(f'Number of training images: {train_images.shape[0]}\\n')\n",
    "print(f'Number of test images: {test_images.shape[0]}\\n')\n",
    "\n",
    "print(f'Image size: {train_images.shape[1:]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHF6g2dQNQVc"
   },
   "source": [
    "There are logically 60,000 training labels and 10,000 test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "GUsWcVaUNVev",
    "outputId": "c13c5423-b779-4b3d-e0a7-54b03b8fc067"
   },
   "outputs": [],
   "source": [
    "print(f'Number of labels: {len(train_labels)}\\n')\n",
    "print(f'Number of test labels: {len(test_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSk_ogerNlq4"
   },
   "source": [
    "Each label is an integer between 0 and 9 corresponding to the 10 class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "jkDXYdJ5N5zk",
    "outputId": "55b5a3c4-aae3-46b0-a5b0-a4a31e0c610e"
   },
   "outputs": [],
   "source": [
    "unique_train_labels = np.unique(train_labels)\n",
    "\n",
    "for label in zip(class_names, train_labels):\n",
    "  label_name, label_num = label\n",
    "  print(f'{label_name}: {label_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVOPKTgdP-nS"
   },
   "source": [
    "To properly train the model, the data must be normalized so each value will fall between 0 and 1. Later on this step will be done inside of the training script but we will show what that process looks like here.\n",
    "\n",
    "The first image shows that the values fall in a range between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "m4VEw8Ud9Quh",
    "outputId": "9942926b-6ee4-4f84-e427-56e67a86d6f8"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dn88kMUcR5G5"
   },
   "source": [
    "To scale the data we divide the training and test values by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VREJVpLRqeP"
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ocp5nOyjSBEe"
   },
   "source": [
    "We plot the first 25 images from the training set to show that the data is in fact in the form we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "id": "-KzmYh0kSLSG",
    "outputId": "8d7c3525-39a0-40c4-a5a7-e63593f9fc88"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igI1dvp7SWWV"
   },
   "source": [
    "## Building a Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ctN1VrL_qnA"
   },
   "source": [
    "Now that we have a good handle on what the data looks like. The first step is to install the Kubeflow Pipelines SDK package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kWutHs306En6",
    "outputId": "f87b05b5-0555-43ba-8dae-04056821fe1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already up-to-date: kfp in ./.local/lib/python3.6/site-packages (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied, skipping upgrade: kfp-server-api<0.6.0,>=0.2.5 in ./.local/lib/python3.6/site-packages (from kfp) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: Deprecated in ./.local/lib/python3.6/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from kfp) (0.8.3)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from kfp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: strip-hints in ./.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.20.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (41.2.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.56.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=14.05.14 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.24.2 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/lib/python3/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.4)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.1.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.6)\n",
      "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media!=0.4.0,<0.5dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests->kubernetes<12.0.0,>=8.0.0->kfp) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kubernetes<12.0.0,>=8.0.0->kfp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa>=3.1.4->google-auth>=1.6.1->kfp) (0.4.7)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage>=1.13.0->kfp) (1.14.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage>=1.13.0->kfp) (3.9.1)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage>=1.13.0->kfp) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage>=1.13.0->kfp) (2019.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBwaov51AFex"
   },
   "source": [
    "Restart the kernel for the installs to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "iUJZqAuN6EoK",
    "outputId": "8d520295-afdd-456f-a6f4-d9a17fda8ce2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMWQNog7AZFP"
   },
   "source": [
    "If the install was successful then you should see:\n",
    "\n",
    "/bin/local/dsl <-- This needs to be corrected as it is not the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmHw4UGN6EoX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/dsl-compile\r\n"
     ]
    }
   ],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "S2pApnUc6Eog",
    "outputId": "356e4d86-0b82-484a-afc1-6da702139ba0"
   },
   "outputs": [],
   "source": [
    "# Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4XMweB5D9SC"
   },
   "source": [
    "Create a client to communicate with the Pipelines API server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "DrYUGGBB6Eoo",
    "outputId": "1b0797c0-7064-4ff4-9efa-0c9096daa783"
   },
   "outputs": [],
   "source": [
    "client = kfp.Client(host='pipelines-api.kubeflow.svc.cluster.local:8888')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8hCvyHloOK6"
   },
   "source": [
    "### Building Container Images using Google Cloud Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSA1PoUGiBkE"
   },
   "source": [
    "Cloud Build is a wonderful service offerred by Google Cloud Platform. It allows you to build containers in the cloud which allows you to build large containers without local compute constraints. Your images are then stored in a secure private Google Container Registry. The only charges that apply are those pertaining to storing the images on Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQv21HxxXzNi"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'manceps-labs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Cha7o46AjaYw",
    "outputId": "f1d90f7f-3299-4adc-ca4e-aea8f1cdebc8"
   },
   "outputs": [],
   "source": [
    "# Enable the Cloud Build and Container Registry APIs\n",
    "!gcloud services enable cloudbuild.googleapis.com containerregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPRVHztomSsy"
   },
   "source": [
    "Since the only services we are using are Cloud Build and Container Registry, set the default credentials to use the default Cloud Build service account. This will narrow the scope of what the application is capable of doing on Google Cloud Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "g5AbFGfvXbkg",
    "outputId": "35aa438c-a740-48bd-ca84-a17a83d636c6"
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$PROJECT_ID\"\n",
    "\n",
    "export SERVICE_ACCOUNT_EMAIL=$(gcloud iam service-accounts list | grep 'cloud-build' | awk '{ print $2 }')\n",
    "\n",
    "gcloud config set project $1\n",
    "\n",
    "gcloud iam service-accounts keys create key.json \\\n",
    "    --iam-account=$SERVICE_ACCOUNT_EMAIL\n",
    "\n",
    "export GOOGLE_APPLICATION_CREDENTIALS=key.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JMdihEbEv-R"
   },
   "source": [
    "Define your local paths where your component files will be stored. In our case we will be creating two components. The first component will be a training component which will pull the data, process it, and train a model. \n",
    "\n",
    "The second component will be a prediction component which will acually be predicting the label of a specified image from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmzYlr236Eoy"
   },
   "outputs": [],
   "source": [
    "TRAINING_PATH = '/tmp/components/mnist_training/'\n",
    "PREDICT_PATH = '/tmp/components/mnist_predict/'\n",
    "DATA_PATH = '/mnt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abg3ycFVFhBC"
   },
   "source": [
    "\n",
    "The following cells define strings that will be written to the training and test *app.py* file. It is recommended to look at the corresponding Fashion MNIST notebook to match the code to understand what was included from there and what needed to be added to ensure that it runs properly during pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpeW1k1o6Eo5"
   },
   "outputs": [],
   "source": [
    "train_file = '''import argparse\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Use the argparse package to define command line arguments.\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--epochs', type=str, required=False, default=10, help='Number of epochs to train the model for.')\n",
    "parser.add_argument('--data-path', type=str, required=False, default='/mnt', help='Absolute path where the persistent volume will be mounted.'\n",
    "parser.add_argument('--model-file', type=str, required=True, help='Name of the model file (ex. model.h5).')\n",
    "    \n",
    "args = parser.parse_args()\n",
    "\n",
    "epochs = parser.epochs\n",
    "data_path = parser.data_path\n",
    "model_file = args.model_file\n",
    "\n",
    "# Download the dataset and split into training and test data. \n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the data so that the values all fall between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model using Keras.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())    \n",
    "\n",
    "# Run a training job with specified number of epochs (Default = 10)\n",
    "model.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "# Evaluate the model and print the results\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Save the model to the designated \n",
    "model.save(model_file)\n",
    "\n",
    "# Save the test_data as a pickle file to be used by the predict component.\n",
    "with open(f'{data_path}/test_data', 'wb') as f:\n",
    "    pickle.dump((test_images,test_labels), f)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pxGKEae6Eo_"
   },
   "outputs": [],
   "source": [
    "predict_file = '''import argparse\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the argparse package to define command line arguments.\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--data-path', type=str, required=False, default='/mnt', help='Absolute path where the persistent volume will be mounted.')\n",
    "parser.add_argument('--image-number', type=int, required=False, default=0, help='Image to predict (0-9999).')\n",
    "parser.add_argument('--model-path', type=str, required=True, help='Name of the saved Keras model file (ex. model.h5).')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "image_number = args.image_number\n",
    "model_path = args.model_path\n",
    "\n",
    "# Load the saved Keras model\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "# Load and unpack the test_data\n",
    "with open('/mnt/test_data','rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "# Separate the test_images from the test_labels.\n",
    "test_images, test_labels = test_data\n",
    "\n",
    "# Define the class names.\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Define a Softmax layer to define outputs as probabilities\n",
    "probability_model = tf.keras.Sequential([model, \n",
    "                                        tf.keras.layers.Softmax()])\n",
    "\n",
    "# Grab an image from the test dataset.\n",
    "img = test_images[image_number]\n",
    "\n",
    "# Add the image to a batch where it is the only member.\n",
    "img = (np.expand_dims(img,0))\n",
    "\n",
    "# Predict the label of the image.\n",
    "predictions = probability_model.predict(img)\n",
    "\n",
    "# Take the prediction with the highest probability\n",
    "prediction = np.argmax(predictions[0])\n",
    "\n",
    "# Retrieve the true label of the image from the test labels.\n",
    "true_label = test_labels[image_number]\n",
    "\n",
    "with open(f'{data_path}/result.txt, 'w') as f:\n",
    "  \"Prediction: {} | Confidence: {:2.0f}% | Actual: {}\".format(class_names[prediction],\n",
    "                                100*np.max(predictions),\n",
    "                                class_names[true_label]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgNVRkRC6EpE"
   },
   "outputs": [],
   "source": [
    "dockerfile ='''FROM docker pull tensorflow/serving:2.2.0-gpu-py3\n",
    "ARG is_predict\n",
    "# Change current working directory.\n",
    "WORKDIR /opt\n",
    "\n",
    "COPY . /opt\n",
    "\n",
    "RUN if ['$is_predict'= True]; then python3 -m pip install numpy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GtEWgeA6EpK"
   },
   "outputs": [],
   "source": [
    "def create_files(path, app_text, dockerfile=dockerfile):\n",
    "    '''\n",
    "    Creates app.py file and Dockerfile at specified path.\n",
    "\n",
    "    Args:\n",
    "      path (str): Destination path where app.py and Dockerfile will be created.\n",
    "      app_text (str): A string to be used as the bodyReturns:\n",
    "        str: User choice.of the app.py file.\n",
    "      dockerfile (str): A string specifying the Dockerfile structure to be used.\n",
    "\n",
    "    Returns:\n",
    "      None\n",
    "    '''\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    with open(f'{path}/app.py', 'w') as f:\n",
    "        f.writelines(app_text)\n",
    "        \n",
    "    with open(f'{path}/Dockerfile', 'w') as f:\n",
    "        f.writelines(dockerfile)\n",
    "\n",
    "    return print('File path: ', path, '\\n' \\\n",
    "                 'Files created: ', os.listdir(path), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYKrdz9mcIjE"
   },
   "source": [
    "Create the files needed to build our component containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "tLPXUCN96EpR",
    "outputId": "bcfafc68-998e-49a6-d4d7-5b415f733aea"
   },
   "outputs": [],
   "source": [
    "create_files(TRAINING_PATH, train_file)\n",
    "create_files(PREDICT_PATH, predict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4jlA_986EpY"
   },
   "outputs": [],
   "source": [
    "def create_image_string(image_name, project_id):\n",
    "    '''\n",
    "    Creates Docker image string.\n",
    "    \n",
    "    Args:\n",
    "      image_name (str): Name for Docker image.\n",
    "      project_id (str): GCP Project ID.\n",
    "\n",
    "    Returns:\n",
    "      str: A GCR Docker image tag.\n",
    "    '''\n",
    "    gcr_image = f\"us.gcr.io/{project_id}/{image_name}:latest\"\n",
    "    print(gcr_image)\n",
    "    return gcr_image\n",
    "\n",
    "\n",
    "def build_docker_image(gcr_image, app_folder):\n",
    "    '''\n",
    "    Builds Docker image using Google Cloud Build and stores in Google Cloud Repository\n",
    "\n",
    "    Args:\n",
    "      gcr_image (str): A Google Container Registry tag for Docker container build.\n",
    "      app_folder (str): Path to app.py and Dockerfile.\n",
    "\n",
    "    Return:\n",
    "      None\n",
    "    '''\n",
    "    cmd = ['gcloud', 'builds', 'submit', '--tag', '--build-arg', 'is_predict=False', gcr_image, app_folder]\n",
    "\n",
    "    if predict_gcr_image.find('train') != -1:\n",
    "        cmd[-3] = 'is_predict=True'\n",
    "    build_log = subprocess.run(cmd, stdout=subprocess.PIPE).stdout[:-1].decode('utf-8')\n",
    "    print(build_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IL43ySo8cbhi"
   },
   "source": [
    "Here we produce the Docker image tags we will use to store our images on Google Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "nXwm3Kdh6Epf",
    "outputId": "69d54a35-f793-44e8-fab5-b581dd1090f7"
   },
   "outputs": [],
   "source": [
    "train_gcr_image = create_image_string('train', PROJECT_ID)\n",
    "predict_gcr_image = create_image_string('predict', PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBNur3S6nHJa"
   },
   "source": [
    "Now we build the train and predict containers we will be using in our Kubeflow Pipeline. Make sure that the message that you receive once the operation is complete signals that the builds were successful before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "8L8UuD1J6Epm",
    "outputId": "99bf2d06-47ae-4a5b-c67e-4d1b66a09593"
   },
   "outputs": [],
   "source": [
    "build_docker_image(train_gcr_image, TRAINING_PATH)\n",
    "build_docker_image(predict_gcr_image, PREDICT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qElGo1akoWzv"
   },
   "source": [
    "### Defining our Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-4CsHA8oecE"
   },
   "source": [
    "Kubeflow Pipelines are created decalaratively. The code is not run until the pipeline is compiled. \n",
    "\n",
    "We first define some environment variables which are to be used as inputs at various points in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0ELDlp7R9fL"
   },
   "outputs": [],
   "source": [
    "# Name of the Keras model that will be saved.\n",
    "MODEL_FILE = 'mnist_model.h5'\n",
    "\n",
    "# An integer representing an image from the test set that the model will attempt to predict the label for.\n",
    "IMAGE_NUMBER = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqwBi1Cro49W"
   },
   "source": [
    "Our next step will be to create the various components that will make up the pipeline. We define our pipeline using the *@dsl.pipeline* decorator at the top.\n",
    "\n",
    "We then define our pipeline function which takes in a number of paramters that will be fed into our various components throughout execution.\n",
    "\n",
    "Here we attach a create a [Persistent Volume Claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) using the [VolumeOp](https://) method to save our data across our components. Note that while this is a great method to use locally, you could also use a cloud bucket for your persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "Langpu8q6Ept",
    "outputId": "30769082-0be7-4db4-cee7-20f298c3b0c3"
   },
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='MNIST Pipeline',\n",
    "   description='A toy pipeline that performs mnist model training and prediction.'\n",
    ")\n",
    "# Define parameters to be fed into pipeline\n",
    "def mnist_container_pipeline(\n",
    "    data_path: str = DATA_PATH\n",
    "    model_file: str = f'{DATA_PATH}/{MODEL_FILE}', \n",
    "    project_id: str = PROJECT_ID,\n",
    "    image_number: int = IMAGE_NUMBER\n",
    "):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWM\n",
    ")\n",
    "    \n",
    "    # Create MNIST training component.\n",
    "    mnist_training_container = dsl.ContainerOp(\n",
    "      name=\"model_training\",\n",
    "      image='us.gcr.io/{}/train:latest'.format(project_id),\n",
    "      command=['python', '/opt/app.py'],\n",
    "      pvolumes={data_path: vop.volume}, # Mount volume to specific data path.\n",
    "      arguments=['--model_file', model_file],\n",
    "    )\n",
    "\n",
    "    # Create MNIST prediction component.\n",
    "    mnist_predict_container = dsl.ContainerOp(\n",
    "        name=\"prediction\",\n",
    "        image='us.gcr.io/{}/predict:latest'.format(project_id),\n",
    "        command=['python', '/opt/app.py'],\n",
    "        pvolumes={data_path: mnist_training_container.pvolume}, # Mount same volume from training component.\n",
    "        arguments=['--image-number', image_number,\n",
    "                   '--model-path', model_file]\n",
    "    )\n",
    "\n",
    "    mnist_result_container = dsl.ContainerOp(\n",
    "        name=\"echo_result\",\n",
    "        image='library/bash:4.4.23',\n",
    "        command=['sh', '-c'],\n",
    "        pvolumes={data_path: mnist_predict_container.pvolume}, # Mount the same volume from the training and predict components.\n",
    "        arguments=['cat', f'{data_path}/results.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1pqw3d0tnth"
   },
   "source": [
    "Finally we feed our pipeline definition into the compiler and run it as an experiment. This will give us 2 links at the bottom that we can follow to the [Kubeflow Pipelines UI](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) where you can check logs, artifacts, inputs/outputs, and visually see the progress of your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "E9NphN8F6Epz",
    "outputId": "4e743723-bfb4-4026-d349-0a4a91c4e034"
   },
   "outputs": [],
   "source": [
    "pipeline_func = mnist_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cFLEOyq6Ep5",
    "outputId": "ac3009e4-4beb-4486-bc0a-ad30dfd2780f"
   },
   "outputs": [],
   "source": [
    "experiment_name = 'minist_kubeflow'\n",
    "\n",
    "arguments = {\"model_file\":f\"{DATA_PATH}/{MODEL_NAME}\",\n",
    "             \"project_id\":PROJECT_ID,\n",
    "             \"image_number\": 1}\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KF Fashion MNIST",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
